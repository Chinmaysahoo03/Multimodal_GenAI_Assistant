{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNMJJbE9bsvQbdv5TkNZtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaysahoo03/Multimodal_GenAI_Assistant/blob/main/multimodal_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I3_Y7E8hgol"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq transformers torch diffusers accelerate pillow moviepy opencv-python\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, TextToVideoSDPipeline\n",
        "from PIL import Image\n",
        "from IPython.display import display, Video\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress minor warnings for cleaner output\n",
        "\n",
        "# Setup Groq + LangChain for Chatbot\n",
        "from google.colab import userdata\n",
        "groq_api = userdata.get('groq_api')  # Add your Groq key in Colab Secrets\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize free Groq model (Gemma-2-9B-IT)\n",
        "llm = ChatGroq(api_key=groq_api, model=\"gemma2-9b-it\", temperature=0.7)\n",
        "\n",
        "# Prompt template for conversational style with history\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"{history}\\nHuman: {input}\\nAssistant:\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Setup Text-to-Image: Stable Diffusion\n",
        "pipe_image = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    pipe_image = pipe_image.to(\"cuda\")\n",
        "    print(\"GPU enabled for faster image generation!\")\n",
        "\n",
        "# Setup Text-to-Video: ModelScope Text-to-Video\n",
        "pipe_video = TextToVideoSDPipeline.from_pretrained(\n",
        "    \"damo-vilab/text-to-video-ms-1.7b\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    variant=\"fp16\"\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    pipe_video = pipe_video.to(\"cuda\")\n",
        "    print(\"GPU enabled for faster video generation!\")\n",
        "else:\n",
        "    print(\"Running on CPU for video generation.\")\n",
        "\n",
        "print(\"Models loaded successfully! Groq for chat, Stable Diffusion for images, ModelScope for videos.\")\n",
        "\n",
        "def generate_chat_response(user_input, history=\"\"):\n",
        "    \"\"\"\n",
        "    Generate a text response using Groq and LangChain.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): User's input query.\n",
        "        history (str): Conversation history.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (response text, updated history)\n",
        "    \"\"\"\n",
        "    input_dict = {\"input\": user_input, \"history\": history}\n",
        "    response = chain.invoke(input_dict)\n",
        "    response_text = response['text'] if 'text' in response else response.get('output', 'No valid response')\n",
        "    new_history = f\"{history}\\nHuman: {user_input}\\nAssistant: {response_text}\"\n",
        "    return response_text.strip(), new_history\n",
        "\n",
        "def generate_image(prompt, negative_prompt=\"blurry, low quality\", num_steps=50):\n",
        "    \"\"\"\n",
        "    Generate an image from a text prompt using Stable Diffusion.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Description of the image to generate.\n",
        "        negative_prompt (str): What to avoid in the image (default: \"blurry, low quality\").\n",
        "        num_steps (int): Number of inference steps (default: 50).\n",
        "\n",
        "    Returns:\n",
        "        Image: Generated PIL image.\n",
        "    \"\"\"\n",
        "    image = pipe_image(\n",
        "        prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_steps\n",
        "    ).images[0]\n",
        "\n",
        "    display(image)  # Shows inline in Colab\n",
        "    image.save(\"generated_image.png\")\n",
        "    print(f\"Image saved as generated_image.png\")\n",
        "    return image\n",
        "\n",
        "def generate_video(prompt, num_frames=16, num_steps=25, fps=8):\n",
        "    \"\"\"\n",
        "    Generate a video from a text prompt using ModelScope Text-to-Video pipeline.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Description of the video to generate.\n",
        "        num_frames (int): Number of frames in the video (default: 16).\n",
        "        num_steps (int): Number of inference steps for generation (default: 25).\n",
        "        fps (int): Frames per second for the output video (default: 8).\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved video file.\n",
        "    \"\"\"\n",
        "    video_frames = pipe_video(\n",
        "        prompt,\n",
        "        num_inference_steps=num_steps,\n",
        "        height=320,  # Adjust resolution as needed\n",
        "        width=512,\n",
        "        num_frames=num_frames\n",
        "    ).frames[0]  # Access the frames array\n",
        "\n",
        "    from diffusers.utils import export_to_video\n",
        "    video_path = \"generated_video.mp4\"\n",
        "    export_to_video(video_frames, video_path, fps=fps)\n",
        "\n",
        "    display(Video(video_path, embed=True))\n",
        "    print(f\"Video saved as {video_path}\")\n",
        "    return video_path\n",
        "\n",
        "# Test standalone functionalities\n",
        "print(\"\\n--- Testing Standalone Chat ---\")\n",
        "test_input = \"Hello, tell me about generative AI.\"\n",
        "response, history = generate_chat_response(test_input)\n",
        "print(f\"User: {test_input}\")\n",
        "print(f\"Bot: {response}\")\n",
        "\n",
        "print(\"\\n--- Testing Standalone Image Generation ---\")\n",
        "test_image_prompt = \"a friendly robot chatting with a human in a futuristic room\"\n",
        "generate_image(test_image_prompt)\n",
        "\n",
        "print(\"\\n--- Testing Standalone Video Generation ---\")\n",
        "test_video_prompt = \"a cat playing with a ball in a park\"\n",
        "generate_video(test_video_prompt)\n",
        "\n",
        "# Interactive multimodal assistant loop\n",
        "print(\"\\nMultimodal AI Assistant Ready! Type 'quit' to exit. Use 'generate image: [description]' for images or 'generate video: [description]' for videos.\")\n",
        "history = \"\"  # Stateful history for Groq\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Check for image generation command\n",
        "    if user_input.lower().startswith('generate image:'):\n",
        "        image_prompt = user_input[15:].strip()  # Extract prompt after \"generate image:\"\n",
        "        print(\"Generating image...\")\n",
        "        generate_image(image_prompt)\n",
        "        continue\n",
        "\n",
        "    # Check for video generation command\n",
        "    if user_input.lower().startswith('generate video:'):\n",
        "        video_prompt = user_input[15:].strip()  # Extract prompt after \"generate video:\"\n",
        "        print(\"Generating video...\")\n",
        "        generate_video(video_prompt)\n",
        "        continue\n",
        "\n",
        "    # Generate text response with Groq\n",
        "    response, history = generate_chat_response(user_input, history)\n",
        "    print(f\"Assistant: {response}\")\n",
        "\n",
        "    # Optional: Offer to generate image or video based on response\n",
        "    gen_choice = input(\"Generate an image (i) or video (v) based on this response? (i/v/n): \")\n",
        "    if gen_choice.lower() == 'i':\n",
        "        image_prompt = response[:100] + \"...\"  # Use response as prompt (truncate if long)\n",
        "        print(\"Generating contextual image...\")\n",
        "        generate_image(image_prompt)\n",
        "    elif gen_choice.lower() == 'v':\n",
        "        video_prompt = response[:100] + \"...\"  # Use response as prompt (truncate if long)\n",
        "        print(\"Generating contextual video...\")\n",
        "        generate_video(video_prompt)\n",
        "\n",
        "# Run a scripted demo to showcase integration\n",
        "demo_inputs = [\n",
        "    \"Hi, what's generative AI?\",\n",
        "    \"Can you describe a scene of AI creating art?\",\n",
        "    \"generate image: AI painting a masterpiece\",\n",
        "    \"generate video: AI robot dancing\"\n",
        "]\n",
        "\n",
        "history = \"\"\n",
        "for user_input in demo_inputs:\n",
        "    if user_input.lower().startswith('generate image:'):\n",
        "        image_prompt = user_input[15:].strip()\n",
        "        print(f\"\\n--- Image Request: {image_prompt} ---\")\n",
        "        generate_image(image_prompt)\n",
        "    elif user_input.lower().startswith('generate video:'):\n",
        "        video_prompt = user_input[15].strip()\n",
        "        print(f\"\\n--- Video Request: {video_prompt} ---\")\n",
        "        generate_video(video_prompt)\n",
        "    else:\n",
        "        print(f\"\\nYou: {user_input}\")\n",
        "        response, history = generate_chat_response(user_input, history)\n",
        "        print(f\"Assistant: {response}\")\n",
        "        print(\"---\")"
      ]
    }
  ]
}