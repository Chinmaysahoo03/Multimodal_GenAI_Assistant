{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe/HkqZKMLtC3UayVrYRB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaysahoo03/Multimodal_GenAI_Assistant/blob/main/Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r3_gHigPiZM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-groq transformers torch diffusers accelerate pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress minor warnings for cleaner output\n",
        "\n",
        "# Setup Groq + LangChain for Chatbot (free, fast LLM)\n",
        "from google.colab import userdata\n",
        "groq_api = userdata.get('groq_api')  # Add your Groq key in Colab Secrets\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize free Groq model (Gemma-2-9B-IT)\n",
        "llm = ChatGroq(api_key=groq_api, model=\"gemma2-9b-it\", temperature=0.7)\n",
        "\n",
        "# Prompt template for conversational style with history\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=\"{history}\\nHuman: {input}\\nAssistant:\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Setup Text-to-Image: Stable Diffusion (free, local, no API key)\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    print(\"GPU enabled for faster generation!\")\n",
        "\n",
        "print(\"Models loaded successfully! Groq for chat, Stable Diffusion for images.\")"
      ],
      "metadata": {
        "id": "bJOYd3WlPjcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_chat_response(user_input, history=\"\"):\n",
        "    # Prepare input as a dictionary for invoke\n",
        "    input_dict = {\"input\": user_input, \"history\": history}\n",
        "\n",
        "    # Use invoke instead of run\n",
        "    response = chain.invoke(input_dict)\n",
        "\n",
        "    # Extract the response (assuming it's in the 'text' key or similar; adjust if needed)\n",
        "    response_text = response['text'] if 'text' in response else response.get('output', 'No valid response')\n",
        "    new_history = f\"{history}\\nHuman: {user_input}\\nAssistant: {response_text}\"\n",
        "    return response_text.strip(), new_history\n",
        "\n",
        "# Test standalone\n",
        "test_input = \"Hello, tell me about generative AI.\"\n",
        "response, history = generate_chat_response(test_input)\n",
        "print(f\"User: {test_input}\")\n",
        "print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "3YWzK7jlPjY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(prompt, negative_prompt=\"blurry, low quality\", num_steps=50):\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_inference_steps=num_steps\n",
        "    ).images[0]\n",
        "\n",
        "    # Display and save\n",
        "    display(image)  # Shows inline in Colab\n",
        "    image.save(\"generated_image.png\")\n",
        "    print(f\"Image saved as generated_image.png\")\n",
        "    return image\n",
        "\n",
        "# Test standalone\n",
        "test_prompt = \"a friendly robot chatting with a human in a futuristic room\"\n",
        "generate_image(test_prompt)"
      ],
      "metadata": {
        "id": "fILFdj3YPjXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive multimodal assistant loop\n",
        "print(\"Multimodal AI Assistant Ready! Type 'quit' to exit. Use 'generate image: [description]' to create an image.\")\n",
        "history = \"\"  # Stateful history for Groq\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Check for image generation command\n",
        "    if user_input.lower().startswith('generate image:'):\n",
        "        image_prompt = user_input[15:].strip()  # Extract prompt after \"generate image:\"\n",
        "        print(\"Generating image...\")\n",
        "        generate_image(image_prompt)\n",
        "        continue  # Skip text response for pure image requests\n",
        "\n",
        "    # Generate text response with Groq\n",
        "    response, history = generate_chat_response(user_input, history)\n",
        "    print(f\"Assistant: {response}\")\n",
        "\n",
        "    # Optional: Offer to generate image based on response\n",
        "    gen_choice = input(\"Generate an image based on this response? (y/n): \")\n",
        "    if gen_choice.lower() == 'y':\n",
        "        image_prompt = response[:100] + \"...\"  # Use response as prompt (truncate if long)\n",
        "        print(\"Generating contextual image...\")\n",
        "        generate_image(image_prompt)"
      ],
      "metadata": {
        "id": "dQc5WJsOPjUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a scripted demo to showcase integration\n",
        "demo_inputs = [\n",
        "    \"Hi, what's generative AI?\",\n",
        "    \"Can you describe a scene of AI creating art?\",\n",
        "    \"generate image: AI painting a masterpiece\"\n",
        "]\n",
        "\n",
        "history = \"\"\n",
        "for user_input in demo_inputs:\n",
        "    if user_input.lower().startswith('generate image:'):\n",
        "        image_prompt = user_input[15:].strip()\n",
        "        print(f\"\\n--- Image Request: {image_prompt} ---\")\n",
        "        generate_image(image_prompt)\n",
        "    else:\n",
        "        print(f\"\\nYou: {user_input}\")\n",
        "        response, history = generate_chat_response(user_input, history)\n",
        "        print(f\"Assistant: {response}\")\n",
        "        print(\"---\")"
      ],
      "metadata": {
        "id": "9NIVr-e0PjSY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}