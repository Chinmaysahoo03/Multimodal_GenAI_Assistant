{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpUSej/hDlTuTABcb5d9Ny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaysahoo03/Multimodal_GenAI_Assistant/blob/main/multimodal_appp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI2kvZ6RisjZ",
        "outputId": "dbd2a448-57c2-4fb4-c8e1-f3a29d1941a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, TextToVideoSDPipeline\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress minor warnings for cleaner output\n",
        "\n",
        "# Setup Groq + LangChain for Chatbot\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Multimodal AI Assistant\")\n",
        "st.markdown(\"\"\"\n",
        "Enter your Groq API key below to enable text generation. Then, use the input field to:\n",
        "- Chat with the assistant\n",
        "- Generate images with: **generate image: [description]**\n",
        "- Generate videos with: **generate video: [description]**\n",
        "\"\"\")\n",
        "\n",
        "# Input field for Groq API key\n",
        "if \"groq_api_key\" not in st.session_state:\n",
        "    st.session_state.groq_api_key = \"\"\n",
        "\n",
        "groq_api_key = st.text_input(\"Enter your Groq API Key:\", type=\"password\", key=\"groq_api_input\")\n",
        "if groq_api_key:\n",
        "    st.session_state.groq_api_key = groq_api_key\n",
        "    st.success(\"Groq API key set successfully!\")\n",
        "else:\n",
        "    st.warning(\"Please enter a valid Groq API key to proceed.\")\n",
        "    st.stop()\n",
        "\n",
        "# Initialize Groq model (Gemma-2-9B-IT)\n",
        "try:\n",
        "    llm = ChatGroq(api_key=st.session_state.groq_api_key, model=\"gemma2-9b-it\", temperature=0.7)\n",
        "    # Prompt template for conversational style with history\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"history\", \"input\"],\n",
        "        template=\"{history}\\nHuman: {input}\\nAssistant:\"\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to initialize Groq model: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Setup Text-to-Image: Stable Diffusion\n",
        "try:\n",
        "    pipe_image = StableDiffusionPipeline.from_pretrained(\n",
        "        \"CompVis/stable-diffusion-v1-4\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        pipe_image = pipe_image.to(\"cuda\")\n",
        "        st.info(\"GPU enabled for faster image generation!\")\n",
        "    else:\n",
        "        st.info(\"Running image generation on CPU.\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load Stable Diffusion model: {e}\")\n",
        "    pipe_image = None\n",
        "\n",
        "# Setup Text-to-Video: ModelScope Text-to-Video\n",
        "try:\n",
        "    pipe_video = TextToVideoSDPipeline.from_pretrained(\n",
        "        \"damo-vilab/text-to-video-ms-1.7b\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        variant=\"fp16\"\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        pipe_video = pipe_video.to(\"cuda\")\n",
        "        st.info(\"GPU enabled for faster video generation!\")\n",
        "    else:\n",
        "        st.info(\"Running video generation on CPU.\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load Text-to-Video model: {e}\")\n",
        "    pipe_video = None\n",
        "\n",
        "if pipe_image and pipe_video:\n",
        "    st.success(\"Models loaded successfully! Ready for chat, image, and video generation.\")\n",
        "\n",
        "def generate_chat_response(user_input, history=\"\"):\n",
        "    \"\"\"\n",
        "    Generate a text response using Groq and LangChain.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        input_dict = {\"input\": user_input, \"history\": history}\n",
        "        response = chain.invoke(input_dict)\n",
        "        response_text = response['text'] if 'text' in response else response.get('output', 'No valid response')\n",
        "        new_history = f\"{history}\\nHuman: {user_input}\\nAssistant: {response_text}\"\n",
        "        return response_text.strip(), new_history\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating text response: {e}\")\n",
        "        return \"Error generating response.\", history\n",
        "\n",
        "def generate_image(prompt, negative_prompt=\"blurry, low quality\", num_steps=50):\n",
        "    \"\"\"\n",
        "    Generate an image from a text prompt using Stable Diffusion.\n",
        "    \"\"\"\n",
        "    if pipe_image is None:\n",
        "        st.error(\"Image generation model not loaded.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        image = pipe_image(\n",
        "            prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_steps\n",
        "        ).images[0]\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        image_path = f\"generated_image_{timestamp}.png\"\n",
        "        image.save(image_path)\n",
        "        return image, image_path\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating image: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def generate_video(prompt, num_frames=16, num_steps=25, fps=8):\n",
        "    \"\"\"\n",
        "    Generate a video from a text prompt using ModelScope Text-to-Video pipeline.\n",
        "    \"\"\"\n",
        "    if pipe_video is None:\n",
        "        st.error(\"Video generation model not loaded.\")\n",
        "        return None\n",
        "    try:\n",
        "        video_frames = pipe_video(\n",
        "            prompt,\n",
        "            num_inference_steps=num_steps,\n",
        "            height=320,\n",
        "            width=512,\n",
        "            num_frames=num_frames\n",
        "        ).frames[0]\n",
        "        from diffusers.utils import export_to_video\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        video_path = f\"generated_video_{timestamp}.mp4\"\n",
        "        export_to_video(video_frames, video_path, fps=fps)\n",
        "        return video_path\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating video: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize session state for history\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state.history = \"\"\n",
        "\n",
        "# Input field for user query\n",
        "user_input = st.text_input(\"Your Input:\", key=\"user_input\")\n",
        "\n",
        "# Buttons for actions\n",
        "col1, col2, col3 = st.columns(3)\n",
        "with col1:\n",
        "    chat_button = st.button(\"Generate Text Response\")\n",
        "with col2:\n",
        "    image_button = st.button(\"Generate Image\")\n",
        "with col3:\n",
        "    video_button = st.button(\"Generate Video\")\n",
        "\n",
        "# Process user input\n",
        "if user_input:\n",
        "    if chat_button or (user_input and not user_input.lower().startswith((\"generate image:\", \"generate video:\"))):\n",
        "        # Handle text response\n",
        "        with st.spinner(\"Generating text response...\"):\n",
        "            response, st.session_state.history = generate_chat_response(user_input, st.session_state.history)\n",
        "            st.write(\"**Assistant Response:**\")\n",
        "            st.write(response)\n",
        "\n",
        "            # Offer to generate image or video based on response\n",
        "            st.write(\"Generate media based on this response?\")\n",
        "            col4, col5 = st.columns(2)\n",
        "            with col4:\n",
        "                if st.button(\"Generate Image from Response\"):\n",
        "                    image_prompt = response[:100] + \"...\" if len(response) > 100 else response\n",
        "                    with st.spinner(\"Generating image...\"):\n",
        "                        image, image_path = generate_image(image_prompt)\n",
        "                        if image:\n",
        "                            st.image(image, caption=\"Generated Image\")\n",
        "                            st.write(f\"Image saved as {image_path}\")\n",
        "                            with open(image_path, \"rb\") as file:\n",
        "                                st.download_button(\"Download Image\", file, file_name=image_path)\n",
        "            with col5:\n",
        "                if st.button(\"Generate Video from Response\"):\n",
        "                    video_prompt = response[:100] + \"...\" if len(response) > 100 else response\n",
        "                    with st.spinner(\"Generating video (this may take a while)...\"):\n",
        "                        video_path = generate_video(video_prompt)\n",
        "                        if video_path:\n",
        "                            st.video(video_path)\n",
        "                            st.write(f\"Video saved as {video_path}\")\n",
        "                            with open(video_path, \"rb\") as file:\n",
        "                                st.download_button(\"Download Video\", file, file_name=video_path)\n",
        "\n",
        "    elif user_input.lower().startswith(\"generate image:\") and image_button:\n",
        "        # Handle direct image generation\n",
        "        image_prompt = user_input[15:].strip()\n",
        "        with st.spinner(\"Generating image...\"):\n",
        "            image, image_path = generate_image(image_prompt)\n",
        "            if image:\n",
        "                st.image(image, caption=\"Generated Image\")\n",
        "                st.write(f\"Image saved as {image_path}\")\n",
        "                with open(image_path, \"rb\") as file:\n",
        "                    st.download_button(\"Download Image\", file, file_name=image_path)\n",
        "\n",
        "    elif user_input.lower().startswith(\"generate video:\") and video_button:\n",
        "        # Handle direct video generation\n",
        "        video_prompt = user_input[15:].strip()\n",
        "        with st.spinner(\"Generating video (this may take a while)...\"):\n",
        "            video_path = generate_video(video_prompt)\n",
        "            if video_path:\n",
        "                st.video(video_path)\n",
        "                st.write(f\"Video saved as {video_path}\")\n",
        "                with open(video_path, \"rb\") as file:\n",
        "                    st.download_button(\"Download Video\", file, file_name=video_path)\n",
        "\n",
        "# Demo section\n",
        "st.subheader(\"Run Demo\")\n",
        "if st.button(\"Run Demo\"):\n",
        "    demo_inputs = [\n",
        "        \"Hi, what's generative AI?\",\n",
        "        \"Can you describe a scene of AI creating art?\",\n",
        "        \"generate image: AI painting a masterpiece\",\n",
        "        \"generate video: AI robot dancing\"\n",
        "    ]\n",
        "    st.session_state.history = \"\"\n",
        "    for user_input in demo_inputs:\n",
        "        st.write(f\"**Demo Input:** {user_input}\")\n",
        "        if user_input.lower().startswith(\"generate image:\"):\n",
        "            image_prompt = user_input[15:].strip()\n",
        "            with st.spinner(\"Generating demo image...\"):\n",
        "                image, image_path = generate_image(image_prompt)\n",
        "                if image:\n",
        "                    st.image(image, caption=f\"Demo Image: {image_prompt}\")\n",
        "                    st.write(f\"Image saved as {image_path}\")\n",
        "                    with open(image_path, \"rb\") as file:\n",
        "                        st.download_button(\"Download Demo Image\", file, file_name=image_path)\n",
        "        elif user_input.lower().startswith(\"generate video:\"):\n",
        "            video_prompt = user_input[15:].strip()\n",
        "            with st.spinner(\"Generating demo video...\"):\n",
        "                video_path = generate_video(video_prompt)\n",
        "                if video_path:\n",
        "                    st.video(video_path)\n",
        "                    st.write(f\"Video saved as {video_path}\")\n",
        "                    with open(video_path, \"rb\") as file:\n",
        "                        st.download_button(\"Download Demo Video\", file, file_name=video_path)\n",
        "        else:\n",
        "            with st.spinner(\"Generating demo text response...\"):\n",
        "                response, st.session_state.history = generate_chat_response(user_input, st.session_state.history)\n",
        "                st.write(\"**Assistant Response:**\")\n",
        "                st.write(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit langchain-groq transformers torch diffusers accelerate pillow moviepy opencv-python pyngrok -q\n",
        "\n",
        "# Import required libraries\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "\n",
        "# Set ngrok authtoken from Colab Secrets\n",
        "ngrok_auth_token = userdata.get('ngrok_auth_token')\n",
        "if not ngrok_auth_token:\n",
        "    print(\"Error: Please set the 'ngrok_auth_token' in Colab Secrets.\")\n",
        "else:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "    # Start Streamlit server in the background\n",
        "    subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "    # Create ngrok tunnel\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Access the Streamlit app at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "collapsed": true,
        "id": "XVW99tIOlJT6",
        "outputId": "705db2ec-ab37-4f0e-bc0f-baceb8eb7070"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-09-20T05:56:05+0000 lvl=warn msg=\"failed to start tunnel\" pg=/api/tunnels id=7f16d5afb93c22a3 err=\"failed to start tunnel: The authtoken credential '32wwqziCfTj28VWfkF0SccsvuIo' has been revoked\\nand is no longer valid.\\r\\n\\r\\nERR_NGROK_300\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokHTTPError",
          "evalue": "ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: The authtoken credential '32wwqziCfTj28VWfkF0SccsvuIo' has been revoked\\nand is no longer valid.\\r\\n\\r\\nERR_NGROK_300\\r\\n\"}}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    631\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 502: Bad Gateway",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1893720473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Create ngrok tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Access the Streamlit app at: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     tunnel = NgrokTunnel(api_request(f\"{api_url}/api/tunnels\", method=\"POST\", data=options,\n\u001b[0m\u001b[1;32m    390\u001b[0m                                      timeout=pyngrok_config.request_timeout),\n\u001b[1;32m    391\u001b[0m                          pyngrok_config, api_url)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(url, method, data, params, timeout, auth)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Response {status_code}: {response_data.strip()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         raise PyngrokNgrokHTTPError(f\"ngrok client exception, API returned {status_code}: {response_data}\",\n\u001b[0m\u001b[1;32m    649\u001b[0m                                     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                                     status_code, e.reason, e.headers, response_data)\n",
            "\u001b[0;31mPyngrokNgrokHTTPError\u001b[0m: ngrok client exception, API returned 502: {\"error_code\":103,\"status_code\":502,\"msg\":\"failed to start tunnel\",\"details\":{\"err\":\"failed to start tunnel: The authtoken credential '32wwqziCfTj28VWfkF0SccsvuIo' has been revoked\\nand is no longer valid.\\r\\n\\r\\nERR_NGROK_300\\r\\n\"}}\n"
          ]
        }
      ]
    }
  ]
}