{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpUSej/hDlTuTABcb5d9Ny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaysahoo03/Multimodal_GenAI_Assistant/blob/main/multimodal_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMcvyRCi73_j"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, TextToVideoSDPipeline\n",
        "from PIL import Image\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # Suppress minor warnings for cleaner output\n",
        "\n",
        "# Setup Groq + LangChain for Chatbot\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Multimodal AI Assistant\")\n",
        "st.markdown(\"\"\"\n",
        "Enter your Groq API key below to enable text generation. Then, use the input field to:\n",
        "- Chat with the assistant\n",
        "- Generate images with: **generate image: [description]**\n",
        "- Generate videos with: **generate video: [description]**\n",
        "\"\"\")\n",
        "\n",
        "# Input field for Groq API key\n",
        "if \"groq_api_key\" not in st.session_state:\n",
        "    st.session_state.groq_api_key = \"\"\n",
        "\n",
        "groq_api_key = st.text_input(\"Enter your Groq API Key:\", type=\"password\", key=\"groq_api_input\")\n",
        "if groq_api_key:\n",
        "    st.session_state.groq_api_key = groq_api_key\n",
        "    st.success(\"Groq API key set successfully!\")\n",
        "else:\n",
        "    st.warning(\"Please enter a valid Groq API key to proceed.\")\n",
        "    st.stop()\n",
        "\n",
        "# Initialize Groq model (Gemma-2-9B-IT)\n",
        "try:\n",
        "    llm = ChatGroq(api_key=st.session_state.groq_api_key, model=\"gemma2-9b-it\", temperature=0.7)\n",
        "    # Prompt template for conversational style with history\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"history\", \"input\"],\n",
        "        template=\"{history}\\nHuman: {input}\\nAssistant:\"\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to initialize Groq model: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Setup Text-to-Image: Stable Diffusion\n",
        "try:\n",
        "    pipe_image = StableDiffusionPipeline.from_pretrained(\n",
        "        \"CompVis/stable-diffusion-v1-4\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        pipe_image = pipe_image.to(\"cuda\")\n",
        "        st.info(\"GPU enabled for faster image generation!\")\n",
        "    else:\n",
        "        st.info(\"Running image generation on CPU.\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load Stable Diffusion model: {e}\")\n",
        "    pipe_image = None\n",
        "\n",
        "# Setup Text-to-Video: ModelScope Text-to-Video\n",
        "try:\n",
        "    pipe_video = TextToVideoSDPipeline.from_pretrained(\n",
        "        \"damo-vilab/text-to-video-ms-1.7b\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        variant=\"fp16\"\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        pipe_video = pipe_video.to(\"cuda\")\n",
        "        st.info(\"GPU enabled for faster video generation!\")\n",
        "    else:\n",
        "        st.info(\"Running video generation on CPU.\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to load Text-to-Video model: {e}\")\n",
        "    pipe_video = None\n",
        "\n",
        "if pipe_image and pipe_video:\n",
        "    st.success(\"Models loaded successfully! Ready for chat, image, and video generation.\")\n",
        "\n",
        "def generate_chat_response(user_input, history=\"\"):\n",
        "    \"\"\"\n",
        "    Generate a text response using Groq and LangChain.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        input_dict = {\"input\": user_input, \"history\": history}\n",
        "        response = chain.invoke(input_dict)\n",
        "        response_text = response['text'] if 'text' in response else response.get('output', 'No valid response')\n",
        "        new_history = f\"{history}\\nHuman: {user_input}\\nAssistant: {response_text}\"\n",
        "        return response_text.strip(), new_history\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating text response: {e}\")\n",
        "        return \"Error generating response.\", history\n",
        "\n",
        "def generate_image(prompt, negative_prompt=\"blurry, low quality\", num_steps=50):\n",
        "    \"\"\"\n",
        "    Generate an image from a text prompt using Stable Diffusion.\n",
        "    \"\"\"\n",
        "    if pipe_image is None:\n",
        "        st.error(\"Image generation model not loaded.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        image = pipe_image(\n",
        "            prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_steps\n",
        "        ).images[0]\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        image_path = f\"generated_image_{timestamp}.png\"\n",
        "        image.save(image_path)\n",
        "        return image, image_path\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating image: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def generate_video(prompt, num_frames=16, num_steps=25, fps=8):\n",
        "    \"\"\"\n",
        "    Generate a video from a text prompt using ModelScope Text-to-Video pipeline.\n",
        "    \"\"\"\n",
        "    if pipe_video is None:\n",
        "        st.error(\"Video generation model not loaded.\")\n",
        "        return None\n",
        "    try:\n",
        "        video_frames = pipe_video(\n",
        "            prompt,\n",
        "            num_inference_steps=num_steps,\n",
        "            height=320,\n",
        "            width=512,\n",
        "            num_frames=num_frames\n",
        "        ).frames[0]\n",
        "        from diffusers.utils import export_to_video\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        video_path = f\"generated_video_{timestamp}.mp4\"\n",
        "        export_to_video(video_frames, video_path, fps=fps)\n",
        "        return video_path\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error generating video: {e}\")\n",
        "        return None\n",
        "\n",
        "# Initialize session state for history\n",
        "if \"history\" not in st.session_state:\n",
        "    st.session_state.history = \"\"\n",
        "\n",
        "# Input field for user query\n",
        "user_input = st.text_input(\"Your Input:\", key=\"user_input\")\n",
        "\n",
        "# Buttons for actions\n",
        "col1, col2, col3 = st.columns(3)\n",
        "with col1:\n",
        "    chat_button = st.button(\"Generate Text Response\")\n",
        "with col2:\n",
        "    image_button = st.button(\"Generate Image\")\n",
        "with col3:\n",
        "    video_button = st.button(\"Generate Video\")\n",
        "\n",
        "# Process user input\n",
        "if user_input:\n",
        "    if chat_button or (user_input and not user_input.lower().startswith((\"generate image:\", \"generate video:\"))):\n",
        "        # Handle text response\n",
        "        with st.spinner(\"Generating text response...\"):\n",
        "            response, st.session_state.history = generate_chat_response(user_input, st.session_state.history)\n",
        "            st.write(\"**Assistant Response:**\")\n",
        "            st.write(response)\n",
        "\n",
        "            # Offer to generate image or video based on response\n",
        "            st.write(\"Generate media based on this response?\")\n",
        "            col4, col5 = st.columns(2)\n",
        "            with col4:\n",
        "                if st.button(\"Generate Image from Response\"):\n",
        "                    image_prompt = response[:100] + \"...\" if len(response) > 100 else response\n",
        "                    with st.spinner(\"Generating image...\"):\n",
        "                        image, image_path = generate_image(image_prompt)\n",
        "                        if image:\n",
        "                            st.image(image, caption=\"Generated Image\")\n",
        "                            st.write(f\"Image saved as {image_path}\")\n",
        "                            with open(image_path, \"rb\") as file:\n",
        "                                st.download_button(\"Download Image\", file, file_name=image_path)\n",
        "            with col5:\n",
        "                if st.button(\"Generate Video from Response\"):\n",
        "                    video_prompt = response[:100] + \"...\" if len(response) > 100 else response\n",
        "                    with st.spinner(\"Generating video (this may take a while)...\"):\n",
        "                        video_path = generate_video(video_prompt)\n",
        "                        if video_path:\n",
        "                            st.video(video_path)\n",
        "                            st.write(f\"Video saved as {video_path}\")\n",
        "                            with open(video_path, \"rb\") as file:\n",
        "                                st.download_button(\"Download Video\", file, file_name=video_path)\n",
        "\n",
        "    elif user_input.lower().startswith(\"generate image:\") and image_button:\n",
        "        # Handle direct image generation\n",
        "        image_prompt = user_input[15:].strip()\n",
        "        with st.spinner(\"Generating image...\"):\n",
        "            image, image_path = generate_image(image_prompt)\n",
        "            if image:\n",
        "                st.image(image, caption=\"Generated Image\")\n",
        "                st.write(f\"Image saved as {image_path}\")\n",
        "                with open(image_path, \"rb\") as file:\n",
        "                    st.download_button(\"Download Image\", file, file_name=image_path)\n",
        "\n",
        "    elif user_input.lower().startswith(\"generate video:\") and video_button:\n",
        "        # Handle direct video generation\n",
        "        video_prompt = user_input[15:].strip()\n",
        "        with st.spinner(\"Generating video (this may take a while)...\"):\n",
        "            video_path = generate_video(video_prompt)\n",
        "            if video_path:\n",
        "                st.video(video_path)\n",
        "                st.write(f\"Video saved as {video_path}\")\n",
        "                with open(video_path, \"rb\") as file:\n",
        "                    st.download_button(\"Download Video\", file, file_name=video_path)\n",
        "\n",
        "# Demo section\n",
        "st.subheader(\"Run Demo\")\n",
        "if st.button(\"Run Demo\"):\n",
        "    demo_inputs = [\n",
        "        \"Hi, what's generative AI?\",\n",
        "        \"Can you describe a scene of AI creating art?\",\n",
        "        \"generate image: AI painting a masterpiece\",\n",
        "        \"generate video: AI robot dancing\"\n",
        "    ]\n",
        "    st.session_state.history = \"\"\n",
        "    for user_input in demo_inputs:\n",
        "        st.write(f\"**Demo Input:** {user_input}\")\n",
        "        if user_input.lower().startswith(\"generate image:\"):\n",
        "            image_prompt = user_input[15:].strip()\n",
        "            with st.spinner(\"Generating demo image...\"):\n",
        "                image, image_path = generate_image(image_prompt)\n",
        "                if image:\n",
        "                    st.image(image, caption=f\"Demo Image: {image_prompt}\")\n",
        "                    st.write(f\"Image saved as {image_path}\")\n",
        "                    with open(image_path, \"rb\") as file:\n",
        "                        st.download_button(\"Download Demo Image\", file, file_name=image_path)\n",
        "        elif user_input.lower().startswith(\"generate video:\"):\n",
        "            video_prompt = user_input[15:].strip()\n",
        "            with st.spinner(\"Generating demo video...\"):\n",
        "                video_path = generate_video(video_prompt)\n",
        "                if video_path:\n",
        "                    st.video(video_path)\n",
        "                    st.write(f\"Video saved as {video_path}\")\n",
        "                    with open(video_path, \"rb\") as file:\n",
        "                        st.download_button(\"Download Demo Video\", file, file_name=video_path)\n",
        "        else:\n",
        "            with st.spinner(\"Generating demo text response...\"):\n",
        "                response, st.session_state.history = generate_chat_response(user_input, st.session_state.history)\n",
        "                st.write(\"**Assistant Response:**\")\n",
        "                st.write(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install streamlit langchain-groq transformers torch diffusers accelerate pillow moviepy opencv-python pyngrok -q\n",
        "\n",
        "# Import required libraries\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "\n",
        "# Set ngrok authtoken from Colab Secrets\n",
        "ngrok_auth_token = userdata.get('ngrok_auth_token')\n",
        "if not ngrok_auth_token:\n",
        "    print(\"Error: Please set the 'ngrok_auth_token' in Colab Secrets.\")\n",
        "else:\n",
        "    ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "    # Start Streamlit server in the background\n",
        "    subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "    # Create ngrok tunnel\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Access the Streamlit app at: {public_url}\")"
      ],
      "metadata": {
        "id": "MaJ_o2Tl78LP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}